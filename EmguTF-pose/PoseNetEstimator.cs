using System;
using System.Drawing;
using Emgu.CV; 
using Emgu.CV.Util;
using Emgu.CV.CvEnum;
using Emgu.CV.Structure;

namespace EmguTF_pose
{
    /// <summary>
    /// A posenet estimator is a deep neural network loaded with Emgu.TF.Lite. It 
    /// generates a 3D tensor of heatmaps and a 3D tensor of offsets when fed with an 
    /// image. The idea is to estimate the keypoint position on a regular grid, then to
    /// translate it to its real location in the input image dimension with an offset vector.
    /// We will neglect the other outputs of the network for now (they are useful for multi-body 
    /// pose estimation only).
    /// 
    /// * The heatmap tensor is a 3D tensor of size resolution x resolution x 17 (number of keypoints)
    ///   where each channel represents the probability of a specified keypoint
    ///   on a regular grid (size resolution). The number 17 is known a-priori with PoseNet.
    ///   
    /// * The offset is a 3D tensor of size resolution x resolution x 34 (twice more channels)
    ///   where channels 0-16 are X axis offset, and channels 17-33 are Y axis offsets
    ///
    /// * For both resolution Resolution = ((InputImageSize - 1) / OutputStride) + 1
    ///   where InputImageSize is input dependent. In our case, we will make sure to resize it
    ///   to 257 x 257 pixels. The output stride is fixed to 32 in our case (.tflite weights for
    ///   other output stides have not been released & output stride 32 is the fastest version). 
    ///   As a result, outputs resolution is equal to 9 here.
    /// 
    /// The workflow is the following:
    ///     posenet(image) --> heatmap, offset --> getkeypoint(heatmap) --> getoffsets(offsets, keypoints)
    /// </summary>
    class PoseNetEstimator : DeepNetworkLite 
    {
        /// <summary>
        /// The number of keypoints we can find.
        /// This is an a-priori knowledge based on the network architecture.
        /// We have 17 keypoints per body to find with PoseNet.
        /// Their names are stored in <see cref="m_keypointsNames"/>, while the 
        /// keypoints themselve are updated in after each forward pass/inference in <see cref="m_keypoints"/>.
        /// </summary>
        public const int m_numberOfKeypoints = 17;

        /// <summary>
        /// The keypoints found with posenet on an input image.
        /// Each keypoint is obtained as the maximum location from the estimated heatmaps stored in <see cref="m_heatmapsChannels"/>.
        /// The number of keypoints we retrieve is given by <see cref="m_numberOfKeypoints"/>.
        /// The name for each keypoint is stored in <see cref="m_keypointNames"/>.
        /// </summary>
        public Point[] m_keypoints = new Point[m_numberOfKeypoints];

        /// <summary>
        /// The name of the keypoints found with posenet. 
        /// </summary>
        public string[] m_keypointName = new string[m_numberOfKeypoints]{
                    "nose", "left eye", "right eye", "left ear", "right ear", "left shoulder",
                    "right shoulder", "left elbow", "right elbow", "left wrist", "right wrist",
                    "left hip", "right hip", "left knee", "right knee", "left ankle", "right ankle"
        };

        /// <summary>
        /// Each keypoint is retrieved from a probabilistic heatmap generated by the DCNN.
        /// Heatmaps are represented through 3D tensor that we convert to an Emgu.CV.Mat in <see cref="Inference(Mat)"/>.
        /// Each channel of the resulting Mat represent one heatmap, therefore standing for one keypoint.
        /// We split the channels and store them in our m_heatmapsChannels to later retrieve keypoint locations.
        /// 
        /// NB: Because the heatmap resolution is smaller than the input image resolution, we need to use
        /// the offset values generated by the network and stored in <see cref="m_offsetsChannels"/>
        /// to display the keypoints.
        /// </summary>
        private VectorOfMat m_heatmapsChannels = new VectorOfMat(m_numberOfKeypoints);

        /// <summary>
        /// For each keypoint obtained from an heatmap channel stored in <see cref="m_heatmapsChannels"/>,
        /// we need to retrieve a translation vector from the output grid resolution to the input
        /// image resolution. These translation vectors are particularly useful for display purpose.
        /// We propose to store all the offsets in a VectorOfMat to use Emgu.CV functions on them.
        /// This storage is achieved by converting the 3D tensors to Emgu.CV Mat in <see cref="Inference(Mat)"/>,
        /// then splitting the Mat along the channels dimension.
        /// </summary>
        private VectorOfMat m_offsetsChannels = new VectorOfMat(m_numberOfKeypoints);

        /// <summary>
        /// Default constructor. It does nothing but allocating memory. 
        /// You need to specify a frozen model path to make it works.
        /// TIP : use the constructor with arguments, this one is useless for now.
        /// </summary>
        public PoseNetEstimator() { }

        /// <summary>
        /// Constructor with arguments interfacing with the base constructor with argument.
        /// </summary>
        /// <param name="frozenModelPath">Path to a PoseNet model saved with tensorflow lite (.tflite file).</param>
        /// <param name="numberOfThreads">Number of threads the neural network will be able to use (default: 2, from base class)</param>
        public PoseNetEstimator(String frozenModelPath,
                             int numberOfThreads) : base(frozenModelPath,
                                                         numberOfThreads)
        {
        }

        /// <summary>
        /// Perform a forward pass on the image using the current PoseNetEstimator.
        /// We assume the PoseNetEstimator was constructed with the constructor with arguments.
        /// </summary>
        /// <param name="image">A RGB image. It will be resized during the inference to match the network's input size.</param>
        /// <returns>An array of 17 points <see cref="m_numberOfKeypoints"/> representing 17 human body keypoints. 
        ///          If the probability of a keypoint is too low (hardcoded threshold for now, see below),
        ///          keypoint is set to Point(-1,-1). The points are returned in the dimension
        ///          of the network's input size (e.g. 257x257). You may need to further interpolate them for display purpose. A useful
        ///          formula is newX = (currentX / currentWidth) * newWidth (same for y , height). Additionaly, you may want to check
        ///          the list of the ordered keypoints. In this function, we enfore lower body part to not be found (i.e., we set the
        ///          points to Point(-1,-1) for all keypoints 12 to 17.
        ///          
        ///          Ordered keypoints list :
        ///          
        /// </returns>
        public Point[] Inference(Emgu.CV.Mat image)
        {
            // Forward pass
            Emgu.TF.Lite.Tensor[] inference_output = InferenceOnImage(image);

            // 1- Converts 3D tensors to Emgu.CV.Mat - 9 is the resolution here
            // Shapes:
            //    inference_output[0] : heatmaps
            //         heatmaps.Dims[0] : batch size = 1
            //         heatmaps.Dims[1] : resolution = W
            //         heatmaps.Dims[2] : resolution = H
            //         heatmaps.Dims[3] : chennels (1/keypoint) = 17 with PoseNet
            //    inference_output[1] : offsets
            //         offsets.Dims[0] : batch size = 1
            //         offsets.Dims[1] : resolution = W 
            //         offsets.Dims[2] : resolution = H 
            //         offsets.Dims[3] : chennels (2/keypoint; 1 for X, 1 for Y) = 32 with PoseNet
            // ------------------------------------------------------------------------
            Emgu.CV.Mat heatmaps_mat = new Emgu.CV.Mat();
            Emgu.CV.Mat offsets_mat = new Emgu.CV.Mat();
            try
            {
                heatmaps_mat = new Mat(9, 9, DepthType.Cv32F, 17, inference_output[0].DataPointer,
                                       sizeof(float) * 3 * inference_output[0].Dims[1]);
                offsets_mat = new Mat(9, 9, DepthType.Cv32F, 34, inference_output[1].DataPointer,
                                      sizeof(float) * 3 * inference_output[1].Dims[1]);
            }
            catch
            {
                Console.WriteLine("Unable to read heatmaps or offsets in PoseNetEstimator. " +
                                  "Return new Point[0] - empty array of Points.");
                return new Point[0];
            }

            // 2 - Split channels and store them in vector of mat
            if (!heatmaps_mat.IsEmpty & !offsets_mat.IsEmpty)
            {
                Emgu.CV.CvInvoke.Split(heatmaps_mat, m_heatmapsChannels);
                Emgu.CV.CvInvoke.Split(offsets_mat,m_offsetsChannels);
            }
            else
            {
                return new Point[0];
            }

            // 3 - Get max prob on heatmap and apply offset :D
            try
            {
                for (var i = 0; i < 11; i++) // 11 and not 17 to keep only upper body keypoints - todo: remove hardcoded
                {
                    var maxLoc = new Point();
                    var minLoc = new Point();
                    double min = 0;
                    double max = 0;

                    Emgu.CV.CvInvoke.MinMaxLoc(m_heatmapsChannels[i], ref min, ref max, ref minLoc, ref maxLoc);

                    if (sigmoid(max) > 0.05) // 0.05 is a fixed probability threshold between 0 and 1 - todo: remove hardcoded
                    {
                        Image<Gray, Single> offset_y = m_offsetsChannels[i].ToImage<Gray, Single>();
                        Image<Gray, Single> offset_x = m_offsetsChannels[i + 17].ToImage<Gray, Single>();
                        var y = offset_y[maxLoc.Y, maxLoc.X];
                        var x = offset_x[maxLoc.Y, maxLoc.X];

                        m_keypoints[i] = new Point((maxLoc.X * 32 + (int)x.Intensity), (maxLoc.Y * 32 + (int)y.Intensity));
                        // 32 is the output stride
                    }
                    else
                    {
                        m_keypoints[i] = new Point(-1, -1);
                    }
                }
            }
            catch
            {
                Console.WriteLine("Error in PoseNetEstimator Inference : unable to decode heatmaps and offsets. " +
                                  "Return new Point[0] - empty array of points.");
                return new Point[0];
            }

            // Dispose
            heatmaps_mat.Dispose();
            offsets_mat.Dispose();
            for(var i=0; i<17; i++)
            {
                m_heatmapsChannels[i].Dispose();
                m_offsetsChannels[i].Dispose();
            }
            return m_keypoints;
        }
    }
}
